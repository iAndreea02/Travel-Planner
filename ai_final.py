# -*- coding: utf-8 -*-
"""Ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oJZViJpCrDbthP6xBu2ZRKRKRrTVTtgC

Inceputuri

# Colectarea de date
"""

import requests
import pandas as pd
import time
import random

api_key = "5ae2e3f221c38a28845f05b6d39ea85b65163d209754a67b5f62156e"

orase = ["Bucharest", "Cluj-Napoca", "Timisoara", "Iasi", "Constanta",
         "Brasov", "Oradea", "Sibiu", "Targu Mures", "Craiova", "Galati", "Suceava"]

toate_locatiile = []

for oras in orase:
    print(f"游늸 Caut 칥n {oras}")

    geo = requests.get(
        f"https://api.opentripmap.com/0.1/en/places/geoname?name={oras}&apikey={api_key}"
    ).json()

    lat, lon = geo.get("lat"), geo.get("lon")

    if not lat or not lon:
        print(f"丘멆잺 Nu am g캒sit coordonate pentru {oras}. Sar peste...")
        continue

    radius_url = f"https://api.opentripmap.com/0.1/en/places/radius?radius=10000&lon={lon}&lat={lat}&limit=100&apikey={api_key}"
    features = requests.get(radius_url).json().get("features", [])

    for f in features:
        xid = f["properties"]["xid"]
        detail_url = f"https://api.opentripmap.com/0.1/en/places/xid/{xid}?apikey={api_key}"
        detail = requests.get(detail_url).json()

        if not detail.get("name"):
            continue

        rate = detail.get("rate", "")
        popularitate = {"3h": 3, "2h": 2, "1h": 1}.get(rate, 0)

        rating_general = round(random.uniform(3.5, 5.0), 1)
        nr_recenzii = random.randint(50, 3000)

        categorii = detail.get("kinds", "")
        categorie = categorii.split(",")[0].replace("_", " ").capitalize() if categorii else "Necunoscut"
        categorie_lc = categorie.lower()

        # Sezon
        if "beaches" in categorie_lc or "coast" in categorie_lc:
            sezon = "Cald"
        elif "ski" in categorie_lc or "mountain" in categorie_lc:
            sezon = "Rece"
        elif "museum" in categorie_lc or "architecture" in categorie_lc:
            sezon = "Oric칙nd"
        else:
            sezon = random.choice(["Cald", "Rece", "Oric칙nd"])

        # Pret estimativ
        if "museum" in categorie_lc:
            pret = random.randint(20, 60)
        elif "amusement" in categorie_lc:
            pret = random.randint(50, 100)
        elif "thermal" in categorie_lc or "spa" in categorie_lc:
            pret = random.randint(70, 150)
        elif "park" in categorie_lc or "nature" in categorie_lc:
            pret = 0
        elif "religious" in categorie_lc or "monastery" in categorie_lc:
            pret = random.choice([0, 10])
        else:
            pret = random.randint(10, 50)

        if pret == 0:
            pret_categorie = "Gratuit"
        elif pret <= 30:
            pret_categorie = "Mic"
        elif pret <= 70:
            pret_categorie = "Mediu"
        else:
            pret_categorie = "Mare"

        # Tip calatorie
        if any(kw in categorie_lc for kw in ["museum", "architecture", "urban"]):
            tip_calatorie = "City Break"
        elif any(kw in categorie_lc for kw in ["park", "nature", "mountain", "lake"]):
            tip_calatorie = "Relaxare"
        elif any(kw in categorie_lc for kw in ["thermal", "spa", "amusement", "beach"]):
            tip_calatorie = "Relaxare"
        elif any(kw in categorie_lc for kw in ["historic", "religious", "monastery"]):
            tip_calatorie = "Circuit"
        else:
            tip_calatorie = random.choice(["City Break", "Circuit", "Relaxare"])

        # Durata minima (numeric캒)
        if tip_calatorie == "City Break":
            durata_minima = 1
        elif tip_calatorie == "Circuit":
            durata_minima = 5
        else:
            durata_minima = 2

        # Cuvinte cheie pentru loca탵ie
        cuvinte_cheie = []

        if "religion" in categorie_lc:
            cuvinte_cheie = ["religios", "spiritual", "biseric캒", "m캒n캒stire", "credin탵캒",]
        elif "historic architecture" in categorie_lc:
            cuvinte_cheie = [ "cl캒diri vechi", "monumente istorice", "istorie", "patrimoniu"]
        elif "historic" in categorie_lc:
            cuvinte_cheie = ["istoric", "cultur캒", "evenimente istorice", "istorie"]
        elif "cinemas" in categorie_lc:
            cuvinte_cheie = ["cinema", "film",  "sal캒 de cinema", "evenimente cinematografice"]
        elif "cultural" in categorie_lc:
            cuvinte_cheie = ["cultur캒", "arte", "expozi탵ie", "festival", "evenimente culturale", "muzic캒", "dans"]
        elif "other" in categorie_lc:
            cuvinte_cheie = ["diverse", "loca탵ie", "interesant", "explorare", "necategorizat", "activit캒탵i diverse"]
        elif "museums" in categorie_lc:
            cuvinte_cheie = ["muzeu", "expozi탵ie", "art캒", "istorie", "cultur캒"]
        elif "architecture" in categorie_lc:
            cuvinte_cheie = ["arhitectur캒", "design", "cl캒diri", "structuri", "construc탵ii", ]
        elif "fountains" in categorie_lc:
            cuvinte_cheie = ["f칙nt칙n캒", "ap캒", "fontan캒", "loc de relaxare", "jocuri de ap캒"]
        elif "palaces" in categorie_lc:
            cuvinte_cheie = ["palat", "monarhie", "regal", "elegan탵캒", "lux"]
        elif "theatres and entertainments" in categorie_lc:
            cuvinte_cheie = ["teatru", "spectacol", "muzic캒", "dans", "entertainment", "divertisment"]
        elif "towers" in categorie_lc:
            cuvinte_cheie = ["turn", "v칙rf", "panoram캒", "priveli탳te"]
        elif "cemeteries" in categorie_lc:
            cuvinte_cheie = ["cemetery", "morm칙nt", "istorie", "cultur캒", "spiritualitate"]
        elif "biographical museums" in categorie_lc:
            cuvinte_cheie = ["muzeu biografic", "via탵a unui om"]
        elif "fortifications" in categorie_lc:
            cuvinte_cheie = ["fortifica탵ie", "castel", "cetate"]
        elif "urban environment" in categorie_lc:
            cuvinte_cheie = ["urban", "ora탳", "str캒zi", "via탵캒 urban캒"]
        elif "gardens and parks" in categorie_lc:
            cuvinte_cheie = ["gr캒dina", "parc", "natur캒", "relaxare", "plimbare"]
        elif "view points" in categorie_lc:
            cuvinte_cheie = ["priveli탳te", "panoram캒", "peisaj"]
        elif "science museums" in categorie_lc:
            cuvinte_cheie = ["muzeu 탳tiin탵ific", "inova탵ie", "tehnologie", "descoperire", "experimente", "inven탵ii"]
        elif "settlements" in categorie_lc:
            cuvinte_cheie = ["a탳ezare", "comunitate", "village", "ora탳", "istorie", "cultura local캒"]
        elif "natural" in categorie_lc:
            cuvinte_cheie = ["natur캒", "conservare", "peisaj natural", "faun캒", "flor캒"]
        elif "beaches" in categorie_lc:
            cuvinte_cheie = ["plaj캒", "soare", "vacan탵캒", "relaxare", "mare", "ocean", "nisip"]
        elif "geological formations" in categorie_lc:
            cuvinte_cheie = ["forme geologice", "munte", "caverne", "pe탳teri"]
        elif "battlefields" in categorie_lc:
            cuvinte_cheie = ["c칙mp de b캒t캒lie", "istorie militar캒", "r캒zboi"]
        elif "bridges" in categorie_lc:
            cuvinte_cheie = ["pod", "construc탵ie", "istoric"]
        else:
            cuvinte_cheie = ["loca탵ie", "descoperire", "activit캒탵i diverse"]

        location = {
            "oras": oras,
            "denumire": detail.get("name"),
            "categorie": categorie,
            "rating_general": rating_general,
            "nr_recenzii": nr_recenzii,
            "pret_categorie": pret_categorie,
            "sezon": sezon,
            "tip_calatorie": tip_calatorie,
            "durata_minima": durata_minima,
            "cuvinte_cheie": cuvinte_cheie,
        }
        toate_locatiile.append(location)
        time.sleep(1)  # Respect캒 limita de API (maxim 1 cerere pe secund캒)

# Creaz캒 DataFrame din lista de loca탵ii
df_locatii = pd.DataFrame(toate_locatiile)

# Salveaz캒 칥n format CSV
df_locatii.to_csv("locatii_turistice.csv", index=False)

# Afi탳eaz캒 primele 5 loca탵ii
print(df_locatii.head())

"""# Descrierea datelor"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("locatii_turistice.csv")
df.head()

print("Randuri: ", len(df), " Coloane:  ", len(df.columns))

print("Numar de elemente:", len(df['rating_general']))
print("Max: ",max(df['rating_general']))
print("Min: ",min(df['rating_general']))
print("Medie: ",df['rating_general'].mean())
print("Mediana:", df['rating_general'].median())
print("Varianta: ", df['rating_general'].std())

plt.hist(df['rating_general'])

"""# Normalizarea 탳i preprocesarea datelor:

### Verifificarea si tratarea datelor null
"""

df.isnull().sum() #numar cate date sunt null

df=df.dropna()
df.isnull().sum()
# sterg datele nulll si verific daca mai am altele

"""## Tratarea duplicatelor"""

print(df.duplicated())  # Afi탳eaz캒 un True pentru r칙ndurile duplicate

"""## Standardizarea"""

print(df['categorie'].unique())  # Afi탳eaz캒 valorile unice din coloana 'categorie'
print(df['tip_calatorie'].unique())  # Afi탳eaz캒 valorile unice din coloana 'tip_calatorie'

"""## Detectarea 탳i Tratarea Valorilor Aberante"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Calcularea IQR pentru coloana 'rating_general'
Q1 = df['rating_general'].quantile(0.25)
Q3 = df['rating_general'].quantile(0.75)
IQR = Q3 - Q1

# Definirea limitelor pentru valorile aberante
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Detectarea valorilor aberante
outliers = df[(df['rating_general'] < lower_bound) | (df['rating_general'] > upper_bound)]

print("Valorile aberante identificate:", outliers)

"""## Transformam String in Numere

### One-Hot Encoding: sezon + tip calatorie+pret
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
import numpy as np


# One-hot encoding pentru sezon
encoder_sezon = OneHotEncoder(sparse_output=False)
sezon_encoded = encoder_sezon.fit_transform(df[['sezon']])

# One-hot encoding pentru tipul de c캒l캒torie
encoder_tip_calatorie = OneHotEncoder(sparse_output=False)
tip_calatorie_encoded = encoder_tip_calatorie.fit_transform(df[['tip_calatorie']])

# One-hot encoding pentru pre탵
encoder_pret = OneHotEncoder(sparse_output=False)
pret_encoded = encoder_pret.fit_transform(df[['pret_categorie']])

# Crearea unui nou DataFrame cu codific캒rile one-hot
df_sezon_encoded = pd.DataFrame(sezon_encoded, columns=encoder_sezon.categories_[0])
df_tip_calatorie_encoded = pd.DataFrame(tip_calatorie_encoded, columns=encoder_tip_calatorie.categories_[0])
df_pret_encoded = pd.DataFrame(pret_encoded, columns=encoder_pret.categories_[0])

# Ad캒ugarea codific캒rilor la DataFrame-ul original
df = pd.concat([df, df_sezon_encoded, df_tip_calatorie_encoded, df_pret_encoded], axis=1)

# Elimin캒m coloanele originale
df = df.drop(['sezon', 'tip_calatorie', 'pret_categorie'], axis=1)
df.head()

"""### Bag of Words (BoW) cuvinte-cheie"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Desc캒rcarea stopwords 탳i wordnet din NLTK
nltk.download('stopwords')
nltk.download('wordnet')

# Ini탵ializare pentru lemmatizare 탳i stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('romanian'))

# Func탵ia de preprocesare a textului
def preprocess_text(text):
    # Conversia la litere mici
    text = text.lower()

    # Eliminarea semnelor de punctua탵ie
    text = re.sub(r'[^\w\s]', '', text)

    # Eliminarea stopwords
    text = " ".join([word for word in text.split() if word not in stop_words])

    # Lemmatizarea
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

# Textul utilizatorului
text_utilizator = "istorie, cladiri vechi,eleganta, monarhie"

# Cuvintele cheie pentru fiecare categorie
cuvinte_cheie = {
    "religion": ["religios", "spiritual", "biseric캒", "m캒n캒stire", "credin탵캒"],
    "historic architecture": ["cl캒diri vechi", "monumente istorice", "istorie", "patrimoniu"],
    "cinemas": ["cinema", "film", "sal캒 de cinema", "evenimente cinematografice"],
    "historic": ["istorie", "cultur캒", "evenimente istorice", "istorie"],
    "cultural": ["cultur캒", "arte", "expozi탵ie", "festival", "evenimente culturale", "muzic캒", "dans"],
    "other": ["diverse", "loca탵ie", "interesant", "explorare", "necategorizat", "activit캒탵i diverse"],
    "museums": ["muzeu", "expozi탵ie", "art캒", "istorie", "cultur캒"],
    "architecture": ["arhitectur캒", "design", "cl캒diri", "structuri", "construc탵ii"],
    "fountains": ["f칙nt칙n캒", "ap캒", "fontan캒", "loc de relaxare", "jocuri de ap캒"],
    "palaces": ["palat", "monarhie", "istorie", "elegan탵캒", "lux"],
    "theatres and entertainments": ["teatru", "spectacol", "muzic캒", "dans", "entertainment", "divertisment", "cultur캒"],
    "towers": ["turn", "v칙rf", "panoram캒", "priveli탳te"],
    "cemeteries": ["cemetery", "morm칙nt", "istorie", "cultur캒", "spiritualitate"],
    "biographical museums": ["muzeu biografic", "via탵a unui om"],
    "fortifications": ["fortifica탵ie", "castel", "cetate", "istorie"],
    "urban environment": ["urban", "ora탳", "via탵캒 urban캒"],
    "gardens and parks": ["gr캒dina", "parc", "natur캒", "relaxare", "plimbare"],
    "view points": ["priveli탳te", "panoram캒", "peisaj", "natur캒"],
    "science museums": ["muzeu stiin탵ific", "inova탵ie", "tehnologie", "descoperire", "experimente", "inven탵ii"],
    "settlements": ["a탳ezare", "comunitate", "village", "ora탳", "istorie", "cultura local캒"],
    "natural": ["natur캒", "conservare", "peisaj natural", "fa콖캒", "flor캒"],
    "beaches": ["plaj캒", "soare", "vacan탵캒", "relaxare", "mare", "ocean", "nisip", "natur캒"],
    "geological formations": ["forme geologice", "munte", "caverne", "pe탳teri", "natur캒"],
    "battlefields": ["c칙mp de b캒t캒lie", "istorie militar캒", "r캒zboi", "istorie"],
    "bridges": ["pod", "construc탵ie", "istorie"]
}

# Preprocesarea textului utilizatorului
text_utilizator_preprocesat = preprocess_text(text_utilizator)

# Preprocesarea cuvintelor cheie pentru fiecare categorie
cuvinte_cheie_preprocesate = {categorie: preprocess_text(" ".join(keywords)) for categorie, keywords in cuvinte_cheie.items()}

# Transform캒 textul utilizatorului 탳i cuvintele cheie 칥ntr-o list캒 de texte
texte_combinate = [text_utilizator_preprocesat] + list(cuvinte_cheie_preprocesate.values())

# Aplicarea CountVectorizer pentru a ob탵ine BoW
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texte_combinate)

# Calcularea similarit캒탵ii cosinus
similaritati = cosine_similarity(X[0:1], X[1:])[0]

# Calcularea probabilit캒탵ii doar pentru similarit캒탵ile mai mari de 0.1
probabilitati = [
    round((similarity + 1) / 2, 2) if similarity > 0.1 else 0
    for similarity in similaritati
]

# Salveaz캒 probabilit캒탵ile 칥ntr-un dic탵ionar
# Dic탵ionare separate pentru similaritate 탳i probabilitate
similaritate_dict = {categorie: similaritate for categorie, similaritate in zip(cuvinte_cheie.keys(), similaritati)}
probabilitate_dict = {categorie: probabilitate for categorie, probabilitate in zip(cuvinte_cheie.keys(), probabilitati)}

# Afi탳캒m dic탵ionarele
print("Dic탵ionarul de similarit캒탵i:")
print(similaritate_dict)

print("Dic탵ionarul de probabilit캒탵i:")
print(probabilitate_dict)


# Afi탳area rezultatelor
# Itereaz캒 prin dic탵ionarele de similaritate 탳i probabilitate
for categorie in similaritate_dict:
    similaritate = similaritate_dict[categorie]
    probabilitate = probabilitate_dict[categorie]

    # Afi탳eaz캒 similaritatea 탳i probabilitatea
    print(f"Similaritatea cu categoria '{categorie}' este {round(similaritate, 2)} (Probabilitate: {probabilitate * 100}%)")

"""## Adaugam coloanele : probabilitate, similaritate, preferinte"""

prag_preferinta=0.2
# Transform캒m valorile din `categorie` 탳i cheile dic탵ionarelor 칥n lowercase 탳i elimin캒m spa탵iile
df['categorie_clean'] = df['categorie'].str.strip().str.lower()

# Ini탵ializ캒m coloanele 'similaritate', 'probabilitate' 탳i 'preferinta' cu valori implicite
df['similaritate'] = 0
df['probabilitate'] = 0
df['preferinta'] = 0

# Verific캒m valorile din df['categorie_clean'] pentru diagnostic
for i, cat in enumerate(df['categorie_clean']):
    # Verific캒m dac캒 categoria se afl캒 칥n ambele dic탵ionare
    if cat in similaritate_dict and cat in probabilitate_dict:
        # Ob탵inem similaritatea 탳i probabilitatea din dic탵ionarele separate
        similaritate = similaritate_dict[cat]
        probabilitate = probabilitate_dict[cat]

        # Set캒m valorile pentru similaritate 탳i probabilitate
        df.at[i, 'similaritate'] = similaritate
        df.at[i, 'probabilitate'] = probabilitate

        # Set캒m preferin탵a pe baza pragului de probabilitate
        if probabilitate >= prag_preferinta:
            df.at[i, 'preferinta'] = 1  # Dac캒 probabilitatea este mai mare dec칙t pragul, set캒m 1
        else:
            df.at[i, 'preferinta'] = 0  # Dac캒 nu, set캒m 0
    else:
        # Dac캒 categoria nu exist캒 칥n dic탵ionare, l캒s캒m valorile implicite (0)
        df.at[i, 'similaritate'] = 0
        df.at[i, 'probabilitate'] = 0
        df.at[i, 'preferinta'] = 0

# 탲tergem coloana temporar캒 'categorie_clean'
df.drop(columns=['categorie_clean'], inplace=True)

# Afi탳캒m DataFrame-ul cu coloanele 'similaritate', 'probabilitate', 탳i 'preferinta'
print(df)

"""### Creare Model+Predictie

### Random Forest
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
import numpy as np

# Cre캒m un DataFrame pentru datele de antrenare
data = []
# Transform캒m valorile din `categorie` 탳i cheile dic탵ionarelor 칥n lowercase 탳i elimin캒m spa탵iile
df['categorie_clean'] = df['categorie'].str.strip().str.lower()
# Complet캒m datele cu valorile de similaritate 탳i probabilitate
for cat in df['categorie_clean']:
    if cat in similaritate_dict and cat in probabilitate_dict:
        similaritate = similaritate_dict[cat]
        probabilitate = probabilitate_dict[cat]
        data.append([similaritate, probabilitate])
    else:
        data.append([0, 0])  # Dac캒 nu exist캒, set캒m la 0

# Cre캒m un DataFrame pentru datele de antrenare
train_df = pd.DataFrame(df, columns=['rating_general', 'similaritate', 'probabilitate'])

# Separ캒m datele 칥n seturi de antrenare 탳i testare
X = train_df[['similaritate', 'probabilitate']]  # Caracteristicile
y = train_df['rating_general']  # Eticheta (ratingul adev캒rat)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ini탵ializ캒m modelul de Random Forest
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_rf.fit(X_train, y_train)

# Predic탵ia pe setul de testare
y_pred_rf = model_rf.predict(X_test)

# Calcul캒m eroarea p캒tratic캒 medie (MSE), R^2 탳i eroarea absolut캒 medie (MAE)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)

# Acurate탵ea: Procentajul de predic탵ii care sunt la o diferen탵캒 de maxim 0.5 fa탵캒 de valorile reale
acuratete_rf = np.mean(np.abs(y_test - y_pred_rf) <= 0.5) * 100  # Dac캒 diferen탵a este sub 0.5

# Precizia: Procentajul de predic탵ii corecte 칥n raport cu predic탵iile f캒cute
precizie_rf = np.mean(np.round(y_pred_rf) == np.round(y_test)) * 100  # Compar캒m valorile rotunjite

# Afi탳캒m rezultatele
print(f"Random Forest - Mean Squared Error (MSE): {mse_rf}")
print(f"Random Forest - R^2: {r2_rf}")
print(f"Random Forest - Mean Absolute Error (MAE): {mae_rf}")
print(f"Random Forest - Acurate탵e: {acuratete_rf}%")
print(f"Random Forest - Precizie: {precizie_rf}%")

# Predic탵ia ratingului pe setul original de date
df['rating_predic탵ie_rf'] = model_rf.predict(X)

# Afi탳캒m rezultatele finale
print(df[['categorie_clean', 'rating_predic탵ie_rf']])

"""### Gradient Boosting Regressor"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
import numpy as np

# Cre캒m un DataFrame pentru datele de antrenare
data = []
# Transform캒m valorile din `categorie` 탳i cheile dic탵ionarelor 칥n lowercase 탳i elimin캒m spa탵iile
df['categorie_clean'] = df['categorie'].str.strip().str.lower()
# Complet캒m datele cu valorile de similaritate 탳i probabilitate
for cat in df['categorie_clean']:
    if cat in similaritate_dict and cat in probabilitate_dict:
        similaritate = similaritate_dict[cat]
        probabilitate = probabilitate_dict[cat]
        data.append([similaritate, probabilitate])
    else:
        data.append([0, 0])  # Dac캒 nu exist캒, set캒m la 0

# Cre캒m un DataFrame pentru datele de antrenare
train_df = pd.DataFrame(df, columns=['rating_general', 'similaritate', 'probabilitate'])

# Separ캒m datele 칥n seturi de antrenare 탳i testare
X = train_df[['similaritate', 'probabilitate']]  # Caracteristicile
y = train_df['rating_general']  # Eticheta (ratingul adev캒rat)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ini탵ializ캒m modelul de Gradient Boosting Regressor
model_gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
model_gb.fit(X_train, y_train)

# Predic탵ia pe setul de testare
y_pred_gb = model_gb.predict(X_test)

# Calcul캒m eroarea p캒tratic캒 medie (MSE), R^2 탳i eroarea absolut캒 medie (MAE)
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)
mae_gb = mean_absolute_error(y_test, y_pred_gb)

# Acurate탵ea: Procentajul de predic탵ii care sunt la o diferen탵캒 de maxim 0.5 fa탵캒 de valorile reale
acuratete_gb = np.mean(np.abs(y_test - y_pred_gb) <= 0.5) * 100  # Dac캒 diferen탵a este sub 0.5

# Precizia: Procentajul de predic탵ii corecte 칥n raport cu predic탵iile f캒cute
precizie_gb = np.mean(np.round(y_pred_gb) == np.round(y_test)) * 100  # Compar캒m valorile rotunjite

# Afi탳캒m rezultatele
print(f"Gradient Boosting - Mean Squared Error (MSE): {mse_gb}")
print(f"Gradient Boosting - R^2: {r2_gb}")
print(f"Gradient Boosting - Mean Absolute Error (MAE): {mae_gb}")
print(f"Gradient Boosting - Acurate탵e: {acuratete_gb}%")
print(f"Gradient Boosting - Precizie: {precizie_gb}%")

# Predic탵ia ratingului pe setul original de date
df['rating_predic탵ie_gb'] = model_gb.predict(X)

# Afi탳캒m rezultatele finale
print(df[['categorie_clean', 'rating_predic탵ie_gb']])